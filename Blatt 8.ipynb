{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Sheet 8\n",
    "\n",
    "## Exercise 8-1: Human Height\n",
    "\n",
    "Assume that the height of a human from a finite population is a Gaussian random variable:\n",
    "\n",
    "$$\n",
    "P_w(x_i) = \\mathcal{N} (x_i;\\mu,\\sigma^2) = \\frac{1}{\\sigma\\sqrt{2\\pi}}exp \\left(-\\frac{(x_i-\\mu)^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "For independent $x_i \\in \\mathbb{R}$ from such a population $w=(\\mu,\\sigma)^T \\in \\mathbb{R}^2$ holds\n",
    "\n",
    "$$\n",
    "P_w(x_1,\\dots,x_N) = \\prod^N_{i=1}P_w(x_i)=\n",
    "\\prod^N_{i=1} \\mathcal{N}(x_i;\\mu,\\sigma^2) = \n",
    "\\frac{1}{(2\\pi\\sigma^2)^{\\frac{N}{2}}}exp\\left(-\\frac{1}{2\\sigma^2}\\sum^N_{i=1}(x_i-\\mu)^2)\\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "breakpoint": false
    }
   },
   "source": [
    "### (a)\n",
    "\n",
    "Determine the maximum likelihood estimator of $P_w(x_1,\\dots,x_N)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "breakpoint": false
    }
   },
   "source": [
    "**Solution:**\n",
    "$$\n",
    "\\begin{align}\n",
    "L(w_i, x_1, x_2, ... x_n) \\\\\n",
    "&= \\prod_{i=1}^N P_w(x_i) \\\\\n",
    "&= \\prod_{i=1}^N \\mathcal{N}(x_i, \\mu, \\sigma^2) \\\\\n",
    "&= l(\\mu, \\sigma) = log L(\\mu, \\sigma, x_1,.... x_n) \\\\\n",
    "&= log \\prod_{i=1}^N P_w(x_i) \\\\\n",
    "&= log 1 - \\log \\cdot (2 \\pi \\sigma^2)^{\\frac{N}{2}} + (-\\frac{1}{2\\sigma^2} \\sum_{i=1}^N (x_i - \\mu)^2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Ableitung bilden\n",
    "$$\n",
    "\\frac{d l(\\mu, \\sigma)}{d \\mu} = 0 - \\frac{1}{2\\sigma^2} \\sum_{i=1}^N 2\\cdot(x_i - \\mu) \\cdot (-1) = \\frac{1}{\\sigma^2} \\sum_{i=1}^N \\cdot(x_i - \\mu)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{1}{\\sigma^2} \\left( (\\sum_{i=1}^N x_i) - N\\cdot\\mu \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{d l(\\mu^{ML}, \\sigma)}{d \\mu^{ML}} \\neq 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{1}{\\sigma^2} \\left( (\\sum_{i=1}^N x_i) - N \\cdot \\mu\\right) = 0 l \\cdot \\sigma^2 \\\\\n",
    "\\sum_{i=1}^N x_i = N \\cdot \\mu^{ML} \\\\\n",
    "\\mu^{ML} = \\frac{1}{N} \\sum_{i=1}^N x_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{d l(\\mu, \\sigma^{ML})}{d \\sigma^{ML}} = -\\frac{N}{2} \\cdot \\frac{1}{2\\pi\\sigma^2} \\cdot 4\\pi\\sigma - (-2) \\cdot \\frac{1}{2\\sigma^3} \\cdot \\sum_{i=1}^N (x_i - \\mu)^2 = -\\frac{N}{\\sigma} + \\frac{1}{\\sigma^3} \\sum_{i=1}^N (x_i - \\sigma) = 0\\\\\n",
    "\\frac{1}{\\sigma^3} \\sum_{i=1}^N (x_i - \\mu)^2 = \\frac{N}{\\sigma}\\\\\n",
    "\\frac{1}{N} \\sum_{i=1}^N (x_i - \\mu)^2 = (\\sigma^{ML})^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "breakpoint": false
    }
   },
   "source": [
    "### (b)\n",
    "\n",
    "Compute the corresponding estimators for the four height datasets in the file _body\\_sizes.txt_ and\n",
    "visualize the respective distributions. How does the estimator reflect the understanding of the underlying\n",
    "data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "breakpoint": false
    }
   },
   "source": [
    "**Solution:**\n",
    "$$\n",
    "\\mu_i^{ML} = (161.5536, 153.7481, 154,5920); \\hat\\mu^{ML} = 156.6312\\\\\n",
    "\\sigma_i^{ML} = (34.67525, 35.48248, 36.18142); \\hat\\sigma^{ML} = 35.61861\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 8-2: Linear Regression with Gaussian Noise\n",
    "\n",
    "Let $D, d_i = (x_i,1,\\dots, x_{i,M}, y_i)^T$ , be a dataset of size $N$ with $M$ features and an output $by$ which depends linearly on X.\n",
    "Due to erroneous measurements the inputs the inputs are noisy, i.e.:\n",
    "\n",
    "$$\n",
    "y_i = x_i^Tw + \\epsilon_i,\n",
    "$$\n",
    "\n",
    "where $\\epsilon_i$ is the noise of data point $i$.\n",
    "Furthermore, assume $\\epsilon$ to be gaussian \n",
    "distributed:\n",
    "\n",
    "$$\n",
    "P(\\epsilon_i) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{1}{2\\sigma^2}e_i^2}.\n",
    "$$\n",
    "\n",
    "Given the variables X and the model w, we can then model the distribution of y as follows:\n",
    "\n",
    "$$\n",
    "P(y_i|x_i,w) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{1}{2\\sigma^2}(y_i-x_i^Tw)^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)\n",
    "\n",
    "Determine the parameter $\\hat{w}$ which maximizes the probability of the training data $P(D|w)$, using the _maximum-likelihood estimator_: $\\hat{w}^{ML} = arg max_w P(D|w)$.\n",
    "You may assume that the w are distributed independently of the input data X."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "breakpoint": false
    }
   },
   "source": [
    "**Solution**:\n",
    "\n",
    "$\\epsilon_i = y_i - x_i^Tw$\n",
    "\n",
    "$L(w) = P(D|w) = P(y,X|w)$\n",
    "\n",
    "gegeben: $P(y|X,w)$\n",
    "\n",
    "$P(y,X|w) = P(y|X,w) \\cdot P(X|w)$\n",
    "\n",
    "da x unabh√§ngig von w\n",
    "$$\n",
    "P(X|w) = P(X)\\\\\n",
    "L(w) = P(y|X,w) \\cdot P(X)\\\\\n",
    "= \\prod_{i=1}^N P(y_i|x_i, w) \\cdot P(x_i) = \\prod_{i=1}^N \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{1}{2\\sigma^2}(y_i-x_i^Tw)^2} \\cdot P(X_i)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "l(w) \\\\\n",
    "&= ln L(w) \\\\\n",
    "&= \\sum_{i=1}^N ln \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{1}{2\\sigma^2}(y_i-x_i^Tw)^2} + ln P(X_i) \\\\\n",
    "&= N\\cdot ln 1 - N \\cdot \\sqrt{2\\pi\\sigma^2} \\cdot \\sum_{i=1}^N 1 - \\frac{1}{\\sigma^2} (y_i - x_i^Tw)^2) + ln(P(x_i)) \\\\\n",
    "&= -\\frac{N}{2} \\cdot ln(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^N (y_i - x_i^Tw)^2 + \\sum ln P(x_i)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{d l(w)}{dw} = -\\frac{1}{2\\sigma^2} \\sum_{i=1}^N 2\\cdot (y_i - x_i^Tw) \\cdot (-x_i) = \\frac{1}{\\sigma^2} \\sum_{i=1}^N x_i \\cdot (y_i - x_i^Tw)\\\\\n",
    "\\frac{d l(w^{ML})}{d w^{ML}} \\neq 0 = \\begin{pmatrix}0\\\\.\\\\.\\\\.\\\\0\\end{pmatrix} = \\frac{1}{\\sigma^2} x^T (y-X_w^{ML})\n",
    "$$\n",
    "\n",
    "$$\n",
    "(=) 0 = X^Ty - X^TXw^{ML}\\\\\n",
    "X^TXw^{ML} = X^Ty\\\\\n",
    "w^{ML} = (X^TX)^{-1} X^Ty\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)\n",
    "\n",
    "A common assumption for the a priori distribution of random variables is:\n",
    "\n",
    "$$\n",
    "P(w) = \\frac{1}{(2\\pi\\alpha^2)^{\\frac{M}{2}}}e^{(-\\frac{1}{2\\alpha^2}\\sum^{M-1}_{j=0}w^2_j)}\n",
    "$$\n",
    "\n",
    "Compute the parameter $\\hat{w}$ which maximizes $P(w)P(D|w)$. Does this give an alternative interpretation to the $\\lambda$-term of the penalized least squares function (PLS)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "breakpoint": false
    }
   },
   "source": [
    "**Solution:**\n",
    "\n",
    "*See photos*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
