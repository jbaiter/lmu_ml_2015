{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this exercise we aim at classifying digits using the famous MNIST digits dataset. The dataset consists of 60000 training images and 10000 test images of handwritten digits. Each image has size 28\\*28, and has assigned a label from zero to nine, denoting the digits value, therefore we can use this dataset for supervised training. Download the MNIST dataset from http://yann.lecun.com/exdb/mnist/. You can download an import script for python from http://g.sweyla.com/blog/2012/mnist-numpy/. Import the dataset and, in order to get a vectorial representation, flatten the input images, such that each image is represented by a 784-dimensional vector. You should also flatten the labels to get a vectorial representation from a one-dimensional matrix, as these representations are not equivalent in theano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os, struct\n",
    "from array import array as pyarray\n",
    "from numpy import append, array, int8, uint8, zeros\n",
    "\n",
    "mnist_path=\"/path/to/mnist\"\n",
    "\n",
    "def load_mnist(dataset=\"training\", digits=np.arange(10), path=\".\"):\n",
    "    \"\"\"\n",
    "    Loads MNIST files into 3D numpy arrays\n",
    "\n",
    "    Adapted from: http://abel.ee.ucla.edu/cvxopt/_downloads/mnist.py\n",
    "    \"\"\"\n",
    "\n",
    "    if dataset == \"training\":\n",
    "        fname_img = os.path.join(path, 'train-images-idx3-ubyte')\n",
    "        fname_lbl = os.path.join(path, 'train-labels-idx1-ubyte')\n",
    "    elif dataset == \"testing\":\n",
    "        fname_img = os.path.join(path, 't10k-images-idx3-ubyte')\n",
    "        fname_lbl = os.path.join(path, 't10k-labels-idx1-ubyte')\n",
    "    else:\n",
    "        raise ValueError(\"dataset must be 'testing' or 'training'\")\n",
    "\n",
    "    flbl = open(fname_lbl, 'rb')\n",
    "    magic_nr, size = struct.unpack(\">II\", flbl.read(8))\n",
    "    lbl = pyarray(\"b\", flbl.read())\n",
    "    flbl.close()\n",
    "\n",
    "    fimg = open(fname_img, 'rb')\n",
    "    magic_nr, size, rows, cols = struct.unpack(\">IIII\", fimg.read(16))\n",
    "    img = pyarray(\"B\", fimg.read())\n",
    "    fimg.close()\n",
    "\n",
    "    ind = [ k for k in range(size) if lbl[k] in digits ]\n",
    "    N = len(ind)\n",
    "\n",
    "    images = zeros((N, rows, cols), dtype=uint8)\n",
    "    labels = zeros((N, 1), dtype=int8)\n",
    "    for i in range(len(ind)):\n",
    "        images[i] = array(img[ ind[i]*rows*cols : (ind[i]+1)*rows*cols ]).reshape((rows, cols))\n",
    "        labels[i] = lbl[ind[i]]\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 1)\n",
      "(10000, 784)\n",
      "(60000, 784)\n",
      "(60000,)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "#Load the images for training and testing\n",
    "images, label = load_mnist(dataset=\"training\" , digits=np.arange(10), path=mnist_path)\n",
    "test_images, test_label = load_mnist(dataset=\"testing\" , digits=np.arange(10), path=mnist_path)\n",
    "print label.shape\n",
    "#Flatten the images to get a vectorial representation\n",
    "images = np.asarray([i.flatten() for i in images])\n",
    "test_images = np.asarray([i.flatten() for i in test_images]).astype(theano.config.floatX)\n",
    "#get the maximum for normalization and normalize to values in range [0,1]\n",
    "maxval = np.max(images)\n",
    "images = (images * (1.0/maxval)).astype(theano.config.floatX)\n",
    "test_images = (test_images * (1.0/maxval))\n",
    "#flatten the labels to get a vectorial representation\n",
    "label = label.flatten()\n",
    "test_label = test_label.flatten()\n",
    "print test_images.shape\n",
    "print images.shape\n",
    "print label.shape\n",
    "print test_label.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the data, we want to define an MLP for classification. Construct the following network:\n",
    "* Input $x$: 768-dimensional (i.e. 768 visible units representing the flattened 28\\*28 pixel images)\n",
    "* 100 hidden units $h$\n",
    "* 10 output units $y$ representing the label, with a value close to one in the $i$-th class representing a high probability of the input representing the digit $i$\n",
    "* Use a batch size of 100 and a learning rate of 2\n",
    "* Train 10 epochs. An epoch corresponds to a training interval where each training image is considered once.\n",
    "* Initialize all input weights of the hidden layer with a random uniform distribution in the range [-0.007,0.007]\n",
    "* Initialize all input weights of the top classification layer with a random uniform distribution in the range [-0.05,0.05]\n",
    "* Initialize all biases with zero\n",
    "* Use a sigmoid activation function for the first layer and softmax for the second layer\n",
    "* Optimize with negative log-likelihood as a cost function.\n",
    "\n",
    "The task should be possible to achieve with the knowledge gained from the exercises last week. If you need additional examples you can borrow some code from the deep learning tutorials on http://deeplearning.net/tutorial/. We recommend however that you construct a minimal version of the network on your own without any class structure to gain better insights into the working of Theano. After defining the necessary variables, the network structure can be defined in three lines of code. Defining the updates and the training function can be written in 8 lines of code, and training can be achieved in an additinal five lines.\n",
    "\n",
    "Computing the loss in this special case where labels are not given as a non-zero entry in a zero-valued vector is not trivial. If ${\\tt hiddenOut}$ is the output of the hidden layer, then the loss can be computed as ${\\tt loss=-T.mean(T.log(hiddenOut)[T.arange(y.shape[0]), y])}$. The mean of this expression simply takes the mean over a minibatch of 100 training examples. $\\tt T.log(hiddenOut)$ takes the logarithm of the outputs of the hidden layer, and $[T.arange(y.shape[0]), y]$ picks the correct entries from the output matrix of size ${\\tt (minibatch, \\#labels)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "\n",
    "learning_rate = T.scalar('lr')\n",
    "index = T.lscalar('index')\n",
    "batch_size = 100\n",
    "train_set_x = theano.shared(value=images, name='train_set_x')\n",
    "train_set_y = T.cast(theano.shared(value=label, name='train_set_y'), 'int32')\n",
    "visible_units=images.shape[1]\n",
    "hidden_units=100\n",
    "output_units=10\n",
    "rng = numpy.random.RandomState(1242)\n",
    "epochs = 10\n",
    "n_train_batches = images.shape[0]/batch_size\n",
    "learn_rate = 2\n",
    "\n",
    "########################################\n",
    "##########INPUTS AND WEIGHTS############\n",
    "########################################\n",
    "x = T.matrix('x')\n",
    "#output\n",
    "y = T.ivector('y')\n",
    "#weights first layer\n",
    "W1 = theano.shared(value=numpy.asarray(rng.uniform(low=-0.007,high=0.007,size=(visible_units,hidden_units)),dtype=theano.config.floatX), name='W1')\n",
    "#bias first layer\n",
    "b1 = theano.shared(value=numpy.zeros((hidden_units,),dtype=theano.config.floatX), name='b1')\n",
    "#weights second layer\n",
    "W2 = theano.shared(value=numpy.asarray(rng.uniform(low=-0.05,high=0.05,size=(hidden_units,output_units)),dtype=theano.config.floatX), name='W2')\n",
    "#bias second layer\n",
    "b2 = theano.shared(value=numpy.zeros((output_units,),dtype=theano.config.floatX), name='b2')\n",
    "\n",
    "########################################\n",
    "##############  NETWORK  ###############\n",
    "########################################\n",
    "layer1out = T.nnet.sigmoid(T.dot(x, W1)+b1)\n",
    "layer2out = T.nnet.softmax(T.dot(layer1out, W2)+b2)\n",
    "loss = -T.mean(T.log(layer2out)[T.arange(y.shape[0]), y])\n",
    "\n",
    "prediction = T.argmax(layer2out)\n",
    "\n",
    "########################################\n",
    "#######  GRADIENT AND UPDATES  #########\n",
    "########################################\n",
    "params = [W1,b1,W2,b2]\n",
    "gradient = T.grad(loss, params)\n",
    "updates = []\n",
    "for p, g in zip(params, gradient):\n",
    "   updates.append((p, p - learning_rate * g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = theano.function([index, learning_rate], loss, updates=updates,\n",
    "     givens = {x: train_set_x[(index * batch_size): ((index + 1) * batch_size)],\n",
    "               y: train_set_y[(index * batch_size): ((index + 1) * batch_size)]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refinement epoch 0, cost  0.444167\n",
      "Refinement epoch 1, cost  0.168001\n",
      "Refinement epoch 2, cost  0.124324\n",
      "Refinement epoch 3, cost  0.100445\n",
      "Refinement epoch 4, cost  0.0843293\n",
      "Refinement epoch 5, cost  0.0723783\n",
      "Refinement epoch 6, cost  0.0629108\n",
      "Refinement epoch 7, cost  0.0551432\n",
      "Refinement epoch 8, cost  0.0485799\n",
      "Refinement epoch 9, cost  0.0429254\n"
     ]
    }
   ],
   "source": [
    "for epoch in xrange(epochs):\n",
    "    # go through training set\n",
    "    c = []\n",
    "    for batch_index in xrange(n_train_batches):\n",
    "        c.append(train(batch_index, learn_rate))\n",
    "    print 'Refinement epoch %d, cost ' % epoch, numpy.mean(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual output class of the network can be easily derived by taking the ${\\tt argmax}_i y_i$. Evaluate your classifier on the test set and calculate your error rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correctly classified: 97.2\n"
     ]
    }
   ],
   "source": [
    "data= T.matrix(name='data')\n",
    "test = theano.function([data], prediction,\n",
    "     givens = {x: data})\n",
    "\n",
    "evaluated = 0.0\n",
    "correct = 0.0\n",
    "for i in range(0,test_images.shape[0]):\n",
    "    out = test(test_images[i].reshape(1,test_images[i].shape[0]))\n",
    "    if out == test_label[i]:\n",
    "        correct = correct+1\n",
    "    evaluated = evaluated +1\n",
    "\n",
    "print \"Correctly classified: \" + str(correct/evaluated*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
