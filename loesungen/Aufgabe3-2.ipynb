{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Theano is a python library allowing the efficient evaluation of mathematical expressions on multi-dimensional arrays [http://deeplearning.net/software/theano/]. Compared to numpy, it has two major advantages:\n",
      "1) Efficient differentiation, which is relevant for the backpropagation step in the context of artificial neural networks\n",
      "2) Hardware transparency: Code can be executed both on the CPU and on the GPU with minor modifications\n",
      "\n",
      "In this exercise we aim at delving into the world of Theano. As a first step we will give a short introduction into the basic concepts of theano. Then we will exploit this knowledge to build a neural network.\n",
      "\n"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Exercise 1) Install Theano [http://deeplearning.net/software/theano/install.html#install]\n",
      "\n",
      "Exercise 2) Get a basic idea of what Theano can do [http://deeplearning.net/software/theano/introduction.html]\n",
      "\n",
      "Exercise 3)\n",
      "When learning how to work with Theano, one needs to get used to the way how mathematical expressions are handled by this library. While python evaluates a mathematically expression as soon as it is executed, Theano builds an operator graph from the involved variables and operator. This graph is optimized and can then be employed to evaluated the underlying expression. Note that these steps take some time; later during execution of complex expressions, this cost will pay off.\n",
      "\n",
      "3.a) Generate two scalar variables x1 and x2. These variables will lated be filled with values."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import theano\n",
      "from theano import tensor as T\n",
      "x1 = T.scalar(name='x1')\n",
      "x2 = T.scalar(name='x2')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 135
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "3.b) From the two variables construct an expression e = x1**2+x1*x2+3. Print the representation of this expression"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "e = x1**2+x1*x2+3\n",
      "print(e)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Elemwise{add,no_inplace}.0\n"
       ]
      }
     ],
     "prompt_number": 136
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "3.c) As a next step, we want to evaluate this function. Generate a theano function f that allows us to do so, it should use the variables x1 and x2 as inputs. Execute the function for x1=2 and x2=3. Print the function's representation."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f = theano.function([x1,x2], e)\n",
      "print(f(2,3))\n",
      "theano.printing.pp(f.maker.fgraph.outputs[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "13.0\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 137,
       "text": [
        "'HostFromGpu(GpuElemwise{Composite{(i0 + sqr(i1) + (i1 * i2))}}[(0, 1)](CudaNdarrayConstant{3.0}, GpuFromHost(x1), GpuFromHost(x2)))'"
       ]
      }
     ],
     "prompt_number": 137
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "3.d) Define two new variables x3 and x4, and redefine f using x3 and x4 as inputs. Hint: use the \"givens\" input parameter of theano.function."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x3 = T.scalar(name='x3')\n",
      "x4 = T.scalar(name='x5') #the name is acutally not important\n",
      "#f1 = theano.function([x3,x4], e) would generate an error as x3 is not part of the operator tree\n",
      "f1 = theano.function([x3,x4], e, givens={x1:x3,x2:x4})\n",
      "print(f(2,3))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "13.0\n"
       ]
      }
     ],
     "prompt_number": 138
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "3.e) Another interesting property of theano functions is that parameters can be updated when executing a function. To test this define a shared variable (theano.shared), set its initial value to zero, and redefine the previous expression by replacing 3 with this shared variable. Generate a theano function from this expression that increases the output by 1 each time the function is called. Call this function several times and interpret the results. While this example is nonesense, the concept of updates allows for easily updating the weights and biases of a neural network with this technique: The theano function is defined on the network loss and updates are conducted depending on the gradient of the current input batch."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "m = theano.shared(np.float32(1.0))\n",
      "es = x1**2+x1*x2+m\n",
      "fs = theano.function([x1,x2], es, updates={(m,m+1)})\n",
      "print(str(fs(0,0)))\n",
      "print(str(fs(0,0)))\n",
      "print(str(fs(0,0)))\n",
      "print(str(fs(0,0)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1.0\n",
        "2.0\n",
        "3.0\n",
        "4.0\n"
       ]
      }
     ],
     "prompt_number": 174
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "3.f) As we have already mentioned, theano also allows differentiation. Write an expression that represents the gradient of e with respect to x1, i.e. the partial derivation of e with respect to x1. Then generate a theano function that allows us to evaluate this expression. Evaluate it at x1=3, x2=1. Check the results by computing the gradient by hand."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Generate a function that represents the gradient of e with respect to x1\n",
      "g = T.grad(e, x1)\n",
      "fg = theano.function([x1,x2], g)\n",
      "print(fg(3,1))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "7.0\n"
       ]
      }
     ],
     "prompt_number": 139
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "3.g) Theano can compute several partial derivatives at the same time. Generate an expression g2 that computes the partial derivatives of e with respect to all free variables of e. How is g2 represented? Test this expression by defining an appropriate theano function."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "g2 = T.grad(e, [x1,x2])\n",
      "print(\"Type of g2: \" + str(type(g2))) #the gradient is a list of partial derivatives for x1 and x2\n",
      "print(\"Size of g2: \" + str(len(g2)))\n",
      "fg2 = theano.function([x1,x2], g2)\n",
      "print(str(fg2(3,1)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Type of g2: <type 'list'>\n",
        "Size of g2: 2\n",
        "[array(7.0, dtype=float32), array(3.0, dtype=float32)]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 165
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "\n",
      "4.a) Broadcasting is an extension of matrix operations simplifying life in machine learning see [http://deeplearning.net/software/theano/tutorial/numpy.html#broadcasting] and [http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html]. This technique is exessively used when working with artificial neural networks in the context of mini-batches. In the context of artificial neural networks broadcasting is used to apply a mathematical expression on a set of input vectors, i.e. a minibatch. We will test this using numpy for the sake of simplicity. Compute A*B and B*A with A=[1,1],[2,2],[3,3],[4,4]] and B=[[2,3]]. Interpret the result."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#this computes the component-wise multiplication for every entry of A and B, broadcasted along the y-axis of A.\n",
      "A = np.array([[1,1],[2,2],[3,3],[4,4]])\n",
      "B = np.array([[2,3]])\n",
      "print(str(A*B)+\"\\n\")\n",
      "print(str(B*A))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 2  3]\n",
        " [ 4  6]\n",
        " [ 6  9]\n",
        " [ 8 12]]\n",
        "\n",
        "[[ 2  3]\n",
        " [ 4  6]\n",
        " [ 6  9]\n",
        " [ 8 12]]\n"
       ]
      }
     ],
     "prompt_number": 141
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "4.b) Compute A.dot(B.T). Interpret the results."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(A.dot(B.T))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 5]\n",
        " [10]\n",
        " [15]\n",
        " [20]]\n"
       ]
      }
     ],
     "prompt_number": 142
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      " 5) Define a Perceptron with 2 inputs and one output using Theano. Use the data, labels and weights from exercise 1.4. Use the sigmoid function from theano as an activation function and set a learning rate of 3. As a cost function use the squared Euclidean loss. Generate an expression for calculating the gradient of this cost function. Based on the gradient and the cost expressions, define a function receiving as input a matrix of feature vectors (a minibatch) and a label of vectors, calculating the cost of these inputs and updating the weights and biases of the neural network at the same time. Finally, train the "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "numpy_rng = numpy.random.RandomState(1242)\n",
      "initial_W = numpy.asarray( [[1],[-1]], dtype=theano.config.floatX)\n",
      "initial_b = numpy.asarray( np.zeros((1,)), dtype=theano.config.floatX)\n",
      "data = [[2,4],[1,0.5],[0.5,1.5],[0,0.5]]\n",
      "output = [[1],[1],[0],[0]]\n",
      "\n",
      "b = theano.shared(value=initial_b, name='bias')\n",
      "W = theano.shared(value=initial_W, name='weight')\n",
      "x = T.matrix(name='input')\n",
      "y = T.nnet.sigmoid(T.dot(x,W)+b)\n",
      "yhat = T.matrix(name='groundtruth')\n",
      "learning_rate = 2\n",
      "\n",
      "cost = T.mean((y-yhat).dot((y-yhat).T))\n",
      "g = T.grad(cost,[b,W])\n",
      "\n",
      "upd = []\n",
      "for param, gparam in zip([b,W], g):\n",
      "   upd.append((param, param - learning_rate * gparam))\n",
      "\n",
      "train = theano.function([x,yhat], cost, updates=upd)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 187
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "counter = 0\n",
      "c= 1\n",
      "while c > 0:\n",
      "    c = np.mean(train(data, output))\n",
      "    counter = counter + 1\n",
      "    print \"Iteration \" + str(counter) + \" complete.\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Iteration 1 complete.\n",
        "Iteration 2 complete.\n",
        "Iteration 3 complete.\n",
        "Iteration 4 complete.\n",
        "Iteration 5 complete.\n"
       ]
      }
     ],
     "prompt_number": 188
    }
   ],
   "metadata": {}
  }
 ]
}