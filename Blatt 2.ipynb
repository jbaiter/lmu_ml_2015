{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Sheet 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercise 2-1: The Perceptron in more than two Dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a)\n",
    "The numbers vom 0 through 9 were represented by pixel arrays in the lecture, the corresponding data matrix can be found in the file _numberMatrix.RData_. Use this data to train a perceptron such that it distinguished between odd and even numbers. Vary w and Î·. Additionally, answer the question if the perceptron learning rule terminates for the problem \"is a multiple of 3\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## b)\n",
    "What is the complexity of training a perceptron for an M -dimensional dataset of with N input patterns?\n",
    "What is the cost of a predicition after having trained the perceptron?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2-2: Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let X be a variable providing the data and its occurrences Y:\n",
    "\n",
    "x | 3 | 4 | 5 | 6 | 7 | 8\n",
    "--|---|---|---|---|---|---\n",
    "y | 150 | 155 | 150 | 170 | 160 | 175"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a)\n",
    "Presume the model exhibits the following linear relation:\n",
    "\n",
    "$$\n",
    "y_i = \\beta_0 + \\beta_1x_i = x^Tw\n",
    "$$\n",
    "\n",
    "Use the least squares-estimator introduced in the lecture to determine $w$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "least squares: $cost(w) = \\sum_i ( y_i - \\hat{y_i} )^2$\n",
    "\n",
    "(what I call $\\hat{y}$ is defined as $f(x_i,w) = Xw$, so for the squared error we get:)\n",
    "\n",
    "$cost(w) = (y-Xw)^T(y-Xw)$\n",
    "\n",
    "with the Values given for x we get:\n",
    "\n",
    "$X = \\begin{pmatrix} 1 & 3 \\\\ 1 & 4 \\\\ 1 & 5 \\\\ 1 & 6 \\\\ 1 & 7 \\\\ 1 & 8 \\end{pmatrix}$\n",
    "\n",
    "(the first column with ones corresponds to the bias (multiplied with $\\beta_0$))\n",
    "\n",
    "we calculate the first derivative for $w$ and set it to $0$ (i.e. minimize the error)\n",
    "\n",
    "$$\n",
    "\\frac{\\partial cost(w)}{\\partial w} = -2X^T(y-Xw) = 0\n",
    "$$\n",
    "\n",
    "When solving for $w$ (i.e. $w_{ls}$) we get:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "-2X^T(y-Xw) &= 0 \\\\\n",
    "2X^Ty + 2X^TXw &= 0 \\\\\n",
    "wX^TXw &= -2X^Ty \\\\\n",
    "w &= \\frac{-2X^Ty}{2X^TX} \\\\\n",
    "w &= \\frac{X^Ty}{X^TX} \\\\\n",
    "w &= (X^TX)^{-1}X^Ty\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 134.85714286]\n",
      " [   4.57142857]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = np.matrix([[1,3], [1,4], [1,5], [1,6], [1,7], [1,8]])\n",
    "\n",
    "y = np.matrix([[150], [155], [150], [170], [160], [175]])\n",
    "\n",
    "# WATCH OUT! For Matrices there is no such thing as division!\n",
    "# We need to multiply with the inverse instead\n",
    "\n",
    "ws_ls = ((X.T*X)**-1 ) * X.T * y\n",
    "\n",
    "print ws_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so we get:\n",
    "\n",
    "$\\hat{y_i} \\approx 134.857 + 4.571x_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_vals = X[:,1]\n",
    "y_vals = [134.857 + 4.571*x for x in x_vals]\n",
    "\n",
    "# TODO: make plotable\n",
    "#plt.plot(x_vals, y_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b)\n",
    "Now, presume the non-linear relation\n",
    "$y_i = \\beta_0 + \\beat_1x_i + \\beta_2x_i^2 = x^Tw\n",
    "and, again, determine w."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 149.5       ]\n",
      " [  -1.32142857]\n",
      " [   0.53571429]]\n"
     ]
    }
   ],
   "source": [
    "# only x has changed (squares of x are added as third column):\n",
    "\n",
    "X = np.matrix([[1,3,9], [1,4,16], [1,5,25], [1,6,36], [1,7,49], [1,8,64]])\n",
    "\n",
    "ws_ls2 = ((X.T*X)**-1 ) * X.T * y\n",
    "\n",
    "print ws_ls2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so we get:\n",
    "\n",
    "$\\hat{y_i} \\approx 149.5 + -1.321x_i + 0.536x_i^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c)\n",
    "How could the empiric quadratic error between model and data be visualized? Explain and sketch your suggestion in two as well as in three dimensions on arbitrary data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d)\n",
    "Which of the models a) and b) is better? Compute the average quadratic error and evaluate the models. How could a better model be realized?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2-3: Regularisation / Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a)\n",
    "What is _overfitting_ and how does it occur?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)\n",
    "How can a model be identified as \"overfitted\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c)\n",
    "How can overfitting be avoided?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2-4: Curse of Dimensionality vs. Kernel Trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a)\n",
    "Explain the term _curse of dimensionality_.\n",
    "When does it occur, how can it be avoided?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)\n",
    "Explain the _Kernel Trick_.\n",
    "How can it be used, what is its connection to the _curse of dimensionality_?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Exercise 2-5: Basis Functions of Neural Networks\n",
    "\n",
    "Given a test vector $x_i$, the output of a neural network is defined as\n",
    "$$\n",
    "f(x_i) = \\sum_{h=0}^{M_\\phi-1}w_h\\phi_h(x_i,v_h).\n",
    "$$\n",
    "\n",
    "The weights of the neurons can be learned by employing the back-propagation rule with sample-based gradient descent. In the lecture neural networks with sigmooid neurons have been introduced, but it is possible to employ different basis functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a)\n",
    "Which properties do these basis functions have to fullfill?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)\n",
    "Can a linear combination $\\phi(x_i, v_h) = z_h = \\sum_{j=0}^M v_{h,j}x_{i,j}$ be suitable for this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c)\n",
    "Is the number of parameters for $\\phi(x_i,v_h$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
